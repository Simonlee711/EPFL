{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775a8a63-df82-4cae-b9d1-f28fcec4f71b",
   "metadata": {},
   "source": [
    "Welcome to the laboratory computers for the course \"Neural signals and signal processing\". The aim of the laboratories is to provide insights on how to analyze other kinds of imaging data: here in particular we will look at functional Near Infrared Spectroscopy (fNRIS) and diffusion-weighted MRI to generate tractography and a structural connectome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2321d9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <p><b>Please remember to run this notebook always with the command: \"fsleyes --notebookFile lab_3_fNIRs_tracto.ipynb\"! </b></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005f9d3-e329-4837-918e-9df0b384489e",
   "metadata": {},
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Neural Signals and Signal Processing (NX-421)</h2>\n",
    "<hr style=\"clear:both\"></hr>\n",
    "<h1><font color='black'>Part 1: Get acquainted with fNIRS data </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ec8bb-fae0-4c7f-8f85-9226daaa50f1",
   "metadata": {},
   "source": [
    "# 1. Vizualisation of neurophysiological signals</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a5f92-f5c6-419f-8e53-5ce8922d4a22",
   "metadata": {},
   "source": [
    "fMRI has become the gold standard for in-vivo visualization of the human brain. Nevertheless, this technique is restricted to a laboratory set-up in which the participant is restricted to a fixed position. In contrast, fNIRS experimental set-up stands out for more ecological validity, thanks to a portable version of the device that allows to perform experiments in realistic environments. Still in comparison with fMRI, fNIRS has a relatively higher temporal resolution of 0,01s that allows to disentangle between fluctuations of oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (HbR). But unlike fMRI magnets, fNIRS optical sensors are limited by a low spatial resolution of 3mm and a penetration depth of 1,5mm which does not allow to probe deep areas of the brain.\n",
    "\n",
    "Given these measurement settings, let's visualize and inspect data generated with fNIRS! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ddcbc-ad58-46b9-99e1-955885332020",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3790ca-1acb-48e1-bb57-61c2d4200991",
   "metadata": {},
   "source": [
    "We will work on a set of hemodynamic data measured on one subject during a finger tapping paradigm with three conditions: 1) Tapping the left thumb to fingers, 2) Tapping the right thumb to fingers and 3) A control when nothing happens. Each tapping lasts 5 seconds and there are 30 trials in each condition.The measurement was performed using fNIRS sensors, which were located over motor areas of the cortex. \n",
    "\n",
    "Data were provided by Luke, R., & McAlpine, D. (2021). fNIRS Finger Tapping Data in BIDS Format (Version v0.0.1) (https://doi.org/10.5281/zenodo.5529797),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e47d01-5980-44d8-80ae-21fb655d082f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eca6c2-6752-42d0-8a72-c641a7172cd4",
   "metadata": {},
   "source": [
    "Load the dataset by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ceb50a-fe37-4943-9d9e-5817f0767cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import mne\n",
    "import mne_nirs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Download measurement\n",
    "fnirs_data_folder = mne.datasets.fnirs_motor.data_path()\n",
    "#Get the path for the measurement folder \n",
    "fnirs_data_folder=op.join(fnirs_data_folder, 'Participant-1')\n",
    "#Load the measurement \n",
    "raw_intensity = mne.io.read_raw_nirx(fnirs_data_folder, verbose=True, preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c16446-b605-4aef-b3d6-c4481d1d2b34",
   "metadata": {},
   "source": [
    "Display and read information on the measurement setting such as the number of channels, the file duration or the sampling frequency by runing the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5c5e7-86c8-4ebd-890d-53720c5b2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display information on measurement setting\n",
    "raw_intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0cf60-9d52-47b2-968e-1119147d627a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Multiple Choice Question:\n",
    "Among the choices below, what is the shape of the measured dataset (number of channels, timepoints) ?\n",
    "   * 1. (17, 98888),\n",
    "   * 2. (56, 23239),\n",
    "   * 3. (56, 78889), or\n",
    "   * 4. (17, 23239)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ba0d68-ea25-4278-8c7c-f45eb31d155d",
   "metadata": {},
   "source": [
    "Verify your answer by applying the function shape() on your dataset. Beforehand, you need to extract the dataset from the measuerment file using the function get_data(). Look at the mne documentation by clicking on the following link: https://mne.tools/stable/generated/mne.io.Raw.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dd59f-1783-4903-89e4-991aaf7e156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d3ad9-382b-49ab-9d4a-4a9becbc7bb9",
   "metadata": {},
   "source": [
    "As another measurement setting, the location of brain sensors is very important to understand the spatial resolution of data. Especially, fNIRS sensors, by only covering predefined regions of the cortex, show a low spatial resolution. \n",
    "\n",
    "Run the next cell to view the locations of sensors over the brain surface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e8f0a-f190-4c2b-8a5b-c556a70e6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View location of sensors over brain surface\n",
    "%matplotlib inline\n",
    "raw_intensity.plot_sensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd23aec-dbc9-458b-a5de-1bd8f32cb952",
   "metadata": {},
   "source": [
    "##### Multiple Choice Question\n",
    "What are the lobes covered by this measurement ?\n",
    "* 1. Temporal lobe\n",
    "* 2. Occipital lobe\n",
    "* 3. Prefrontal lobe \n",
    "* 4. Parietal lobe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c02f5-2d5a-44e7-9c09-9b17f0690c1b",
   "metadata": {},
   "source": [
    "Let's now have a look at the experimental design used for this measurement.\n",
    "\n",
    "The experimental designs most commonly employed by auditory fNIRS researchers are block- and event-related designs. Event-related design refers to multiple stimuli that are assumed to occur instantaneously and have randomized time between events. In an event-related finger tapping experiment, the participant would tap a button with the finger approximately every 20 seconds. In a block design, the finger tapping would be performed continuously during a certain time interval and would be followed by a short block of rest.\n",
    "\n",
    "In turn, the choice of the experimental design depends on a range of factors, including the statistical power of the protocol, the duration of the experiment, and whether the design provides the flexibility to study the effect of interest. While the block design might lead to higher detection power, it can also induce learning and boredom effects which may bias the results. On the other hand, event-related designs reduce the effects of learning, boredom and other events unrelated to the task while exhibiting loss in detection power. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6d506-59bc-4300-b1ec-e0b55747c4f7",
   "metadata": {},
   "source": [
    "Run the next cell to display the sequence of events implemented in this measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf4725-0aee-46a5-a0f9-8c4ba23aa854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the event annotations\n",
    "events, event_dict=mne.events_from_annotations(raw_intensity,verbose=False)\n",
    "#Label the events\n",
    "event_dict={'Control':1,'Tapping/Left':4,'Tapping/Right':3,'ExperimentEnds':2}\n",
    "#Display the sequence of events \n",
    "plt.rcParams[\"figure.figsize\"]=(10,6)\n",
    "mne.viz.plot_events(events,event_id=event_dict,sfreq=raw_intensity.info['sfreq']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213d4ad-e49f-4cf3-b4ab-c4ef1cb6ef0a",
   "metadata": {},
   "source": [
    "As you may observe, each individual stimulus is separated to the others by long interstimulus interval. We can thus conclude that an event-related design was used for the studied measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efc021-9a54-4deb-adb8-232f6e0a6729",
   "metadata": {},
   "source": [
    "##### Question\n",
    "Roughly, what is the average interstimulus interval of this design ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb2a3f7-feec-4b81-8128-9ed70614ab17",
   "metadata": {},
   "source": [
    "Let's now vizualise the data. Run the next cell to display data obtained with the fNIRS system. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ec009-9a2d-47e3-98a9-7c9b25862b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a figure window\n",
    "plt.rcParams[\"figure.figsize\"]=(40,40)\n",
    "#Plot a time-series of light intensity \n",
    "raw_intensity.plot(duration=100,n_channels=len(raw_intensity.ch_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9737f-d7b7-42f4-9c34-59b0e9afd454",
   "metadata": {},
   "source": [
    "The near-infrared light sent by a source (labelled \"S\" in our system) is absorbed by the HbO and HHb with a maximal absorption observed at the wavelengths 760nm and 850nm, respectively. The amounts of light absorbed by the two molecules are then detected by a detector  (labelled \"D\" in our system). As a consequence, each channel provides with two time-series of concentration changes, one for the HbO and the other for the HHb. \n",
    "\n",
    "Data are thus initially provided in a raw format of light intensity. This can be converted using the Beer-Lambert law (see equation below), into an optical density which in turn can be translated into concentrations change of HbO and HbR, as proxies of the brain activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1547df8e-0e72-45ec-8e38-4ac81da9f2fd",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    log_{10}(\\frac{I_0}{I})=\\epsilon l c\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e0262-9fb3-4f55-9644-d980fa138240",
   "metadata": {},
   "source": [
    "With $OD_\\lambda$, $I_o$, I, $\\epsilon$, c and l respectively corresponding to the absorbance, the incident light intensity, the transmitted light intensity, the moral absorption coefficient (in $M^-1 cm^-1$), the molar concentration (in M) and the optical pathlength for a given wavelength $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291f369-529e-4c2e-bec1-bb9fc263046a",
   "metadata": {},
   "source": [
    "But since the human tissue is a strong scattering medium for the near-infrared light, the Beer–Lambert law cannot be directly applied to biological tissue. Subsequently, a modified version of the Beer-Lambert law that takes into account the light scattering has been proposed (see equation below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d24940-f47a-4a8a-8d50-3a09392a6f10",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "OD_\\lambda= log\\frac{I_o}{I}=\\epsilon_\\lambda lcB + OD_{R\\lambda}\n",
    "\\end{equation}\n",
    "\n",
    "With B and $OD_{R\\lambda}$ respectively corresponding to the pathlength correction factor and the oxygen independent light losses due to scattering in the tissue. For more information on the equation, see the reference: Delpy DT, Cope M, Zee P van der, Arridge S, Wray S, Wyatt J. Estimation of optical pathlength \n",
    "through tissue from direct time of flight measurements. Phys Med Biol 1988; 33: 1433-1442."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7931b-3449-4789-8ca1-49f54afa377e",
   "metadata": {},
   "source": [
    "Run the next cell to convert raw intensity into optical density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e624f-b7bb-4f19-8da6-6480a56cbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert raw intensity into optical density\n",
    "raw_od=mne.preprocessing.nirs.optical_density(raw_intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dbf9d2-eb2d-4f6e-9114-a1afb0c286f3",
   "metadata": {},
   "source": [
    "And then, run the cell below to convert optical density into concentration change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957ebae-707c-46b7-b342-41d81f33356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required beer-lambert law related function\n",
    "from mne.preprocessing.nirs import beer_lambert_law\n",
    "#Convert optical density into concentration change\n",
    "raw_haemo=beer_lambert_law(raw_od)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587aa5b2-8603-4b11-8078-4036d24ec130",
   "metadata": {},
   "source": [
    "Now let's check the quality of the converted data. \n",
    "\n",
    "As highly perfused tissues, extra-cortical compartments such as the scalp, the skin, the meninges, or the cerebrospinal fluid can absorb the near-infrared light emitted by fNIRS sensors. In turn, the signal measured with fNIRS will confound changes of HbO and HbR concentrations that are evoked by a neural activation in the cortex or by intrinsic mechanism in extra-cortical compartments. For instance, a common change observed in fNIRS signal is the intrinsic variation of cardiac rhythm occurring in extra-cortical tissues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f76c2-eb9c-46e0-9666-e6d0315dac7c",
   "metadata": {},
   "source": [
    "Despite the presence of cardiac oscilliations in the signal is undesirable, it can be used wisely. In particular, the amount of physiological noise measured in a signal has been shown to reflect the optical coupling between the measuring channel and the scalp. Subsequently, a measure of the prominence of the cardiac oscilliations has been established to assess the scalp coupling index (SCI). For more information on the metric computation, see the references:\n",
    "* Pollonini L et al., “PHOEBE: a method for real time mapping of optodes-scalp coupling in functional near-infrared spectroscopy” in Biomed. Opt. Express 7, 5104-5119 (2016). \n",
    "* Hernandez, Samuel Montero, and Luca Pollonini. \"NIRSplot: a tool for quality assessment of fNIRS scans.\" Optics and the Brain. Optical Society of America, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ea886-1645-4756-8987-a456b585ed20",
   "metadata": {},
   "source": [
    "Run the next cell to compute the SCI and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ab6fa-8b95-4790-a94c-ec654b4fcfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the SCI\n",
    "%matplotlib inline\n",
    "sci=mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "#Plot the SCI\n",
    "fig,ax=plt.subplots()\n",
    "ax.hist(sci)\n",
    "ax.set(xlabel='Scalp Coupling Index',ylabel='Count',xlim=[0, 1]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e63e8a-62d5-4db1-9a3f-f557a16a6157",
   "metadata": {},
   "source": [
    "##### Question\n",
    "What is the number of bad channels (SCI < 6) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90266f9a-fdc6-4d76-9820-07fe8ea088a9",
   "metadata": {},
   "source": [
    "Run the next cell to mark the bad channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f565f-e54c-42be-b97f-44c9ad792a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "raw_od.info['bads'] = list(np.compress(sci < 0.6, raw_od.ch_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba952ae1-d0dc-4962-89bf-8a3aa842eea4",
   "metadata": {},
   "source": [
    "Let's now detect and correct the remaining component of the signal that originates from the cardiac oscillations. \n",
    "\n",
    "The cardiac oscilliations represent the major frequency component of the signal. \n",
    "\n",
    "Run the next cell to do decompose the power of the signal into its frequency components. Can you detect the frequency component corresponding to the cardiac oscillations ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2be7f-45ec-4eb5-b430-7db01d3395ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the hemoglobin concentration change\n",
    "fig = raw_haemo.plot_psd(average=False)\n",
    "fig.suptitle('Before filtering', weight='bold', size='x-large')\n",
    "fig.subplots_adjust(top=0.88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df54679-bb1a-4005-bb96-248cd96b2b8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "The power spectrum gives a frequency peak at 1,25 Hz which would correspond to approximately 1 beat / second. You may notice that this frequency corresponds to the cardiac rhythm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544417a9-64d9-4bc6-9624-63131941bd6b",
   "metadata": {},
   "source": [
    "Fortunately, the noise induced by cardiac oscillations can be removed by filtering, since the corresponding frequency band is well distinguished from that of hemodynamic responses (< 0,5Hz). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ac0a6-3c4b-4e03-b7ac-389dc8a01cb8",
   "metadata": {},
   "source": [
    "Based on the above information, apply a low-pass filter to only retain the hemodynamic responses in your signal. You may especially need to read the documentation on filters proposed by mne library (https://mne.tools/stable/generated/mne.filter.filter_data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7add7bc-d02e-4c74-98d6-4512eaa7ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_haemofiltered = #Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd00034-147c-46d0-80cc-c31d1e2a2332",
   "metadata": {},
   "source": [
    "Run the next cell to plot your filtered signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f388af-7022-4e96-bd80-c84625f06a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the hemoglobin concentration change\n",
    "fig = raw_haemofiltered.plot_psd(average=False)\n",
    "fig.suptitle('After filtering', weight='bold', size='x-large')\n",
    "fig.subplots_adjust(top=0.88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ff740-756b-44cc-8c8b-3074c0e5dc6c",
   "metadata": {},
   "source": [
    "Let's now vizualise the data. Run the next cell to display the preprocessed time-series of one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067e665-7722-4107-ba13-694b221506af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the time-series of one channel\n",
    "plt.rcParams[\"figure.figsize\"]=(40,40)\n",
    "raw_haemofiltered.pick(picks=[0,1]).plot(duration=100,n_channels=len(raw_intensity.ch_names),scalings=dict(hb0=1e-5,hbr=1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136f23d",
   "metadata": {},
   "source": [
    "<h1><font color='black'>Part 2: diffusion data and tractography generation </font></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead7e7b",
   "metadata": {},
   "source": [
    "The preprocessing in DTI involves similar steps to what you saw in fMRI. We will thus tackle specific different steps to not repeat ourselves too much:\n",
    "<img src=\"imgs/DTI_preprocessing.png\" />\n",
    "<p  style=\"text-align: center;\"><i>Image from <a href=\"https://www.researchgate.net/publication/311246309_Imaging_analysis_of_Parkinson's_disease_patients_using_SPECT_and_tractography\">Son, Seong-Jin, Mansu Kim, and Hyunjin Park. \"Imaging analysis of Parkinson’s disease patients using SPECT and tractography.\" Scientific reports 6.1 (2016): 1-11.</a></i></p>\n",
    "\n",
    "In other words, we will have you look at an example to generate tractogram. Here, a tractogram will be generated using a deterministic algorithm EuDX.  \n",
    "  \n",
    "Diffusion tensor imaging (DTI) is one of the most popular MRI techniques to describe the orientation of white matter fibers in brain research. The process of fiber tracking is called tractography. It allows for a virtual dissection and three-dimensional representation of white matter tracts. \n",
    "While we could still use FSL for the task, we will have you use <a href=\"https://dipy.org/\">DIPY</a>, a Python package for computational neuroanatomy mainly focusing on diffusion MRI analysis.  \n",
    "<br>\n",
    "To generate a tractogram, we need to track the fibers, which is called fiber tracking.\n",
    "<br>\n",
    "Local fiber tracking is used to model white matter fibers by creating streamlines from local directional information. In order to perform local fiber tracking, you will apply the following three steps:\n",
    "<p >\n",
    "    <ol>\n",
    "        <li style=\"font-size: 15px;\">Extract directions from diffusion data</li> \n",
    "        <li style=\"font-size: 15px;\">Identify when the tracking must stop</li>  \n",
    "        <li style=\"font-size: 15px;\">Select a set of locations from which to begin tracking</li>\n",
    "    </ol>\n",
    "</p>\n",
    "Combining them will help you obtain a tractography reconstruction!\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdbd89",
   "metadata": {},
   "source": [
    "## 0. An additional package\n",
    "We require two additional packages for this lab to work! They are only for visualization.\n",
    "To install them, you will need to use the following commands:\n",
    "```\n",
    "conda install -c conda-forge fury\n",
    "conda install -c conda-forge ipyvtklink \n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582b7ba",
   "metadata": {},
   "source": [
    "### 1. Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a261ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.core.gradients import gradient_table\n",
    "from dipy.data import get_fnames\n",
    "from dipy.io.gradients import read_bvals_bvecs\n",
    "from dipy.io.image import load_nifti, load_nifti_data\n",
    "\n",
    "hardi_fname, hardi_bval_fname, hardi_bvec_fname = get_fnames('stanford_hardi')\n",
    "label_fname = get_fnames('stanford_labels')\n",
    "\n",
    "data, affine, hardi_img = load_nifti(hardi_fname, return_img=True)\n",
    "labels = load_nifti_data(label_fname)\n",
    "bvals, bvecs = read_bvals_bvecs(hardi_bval_fname, hardi_bvec_fname)\n",
    "gtab = gradient_table(bvals, bvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data.shape: ',data.shape)\n",
    "print('affine.shape: ',affine.shape)\n",
    "print('hardi_img.shape: ',hardi_img.shape)\n",
    "\n",
    "print('labels.shape: ',labels.shape)\n",
    "print('bvals.shape: ',bvals.shape)\n",
    "print('bvecs.shape: ',bvecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d842124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "\n",
    "\n",
    "print_dir_tree(op.split(hardi_fname)[0], max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347cc3fe",
   "metadata": {},
   "source": [
    "### 2. Get the directions from the diffusion data set\n",
    "\n",
    "#### 1. Defining the white matter region.\n",
    "Before all else, you will need to visualize the labels above. Run the following cell to load the labels on FSLeyes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d852ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "load(label_fname)\n",
    "displayCtx.getOpts(overlayList[0]).cmap = 'brain_colours_spectrum'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f992f",
   "metadata": {},
   "source": [
    "Based on the values you read within, can you please fill in the cell below with the label corresponding to white matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d29fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_matter_value = ??? # Fill with the value you read in FSLeyes for white matter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493caf5f",
   "metadata": {},
   "source": [
    "Great, let's now visualize the result of the mask, shall we? For this, let's generate the mask we obtained from above, using our best pal fslmaths ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_name = 'extracted_wm'\n",
    "cmd = 'fslmaths ' + label_fname + ' -thr ' + str(white_matter_value) + ' -uthr ' + str(white_matter_value) + ' -bin ' + output_name\n",
    "os.system(cmd)\n",
    "load(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948b411",
   "metadata": {},
   "source": [
    "That's nice, but we are missing something, aren't we? Look in the middle, there is a clear gap in purple. Why is that? If you have on top, you'll see it has a different label: 2. This is because this region is white matter, but it is also a sagittal slice of the **corpus callosum**. So we need to do something a bit different. Can you think of a way to modify the above fslmaths command to include both the white matter and the slice of corpus callosum ? :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_threshold = ??? # Select the lower bound to include both white matter and corpus callosum slice\n",
    "upper_threshold = ?? # Select the upper bound to include both white matter and corpus callosum slice\n",
    "output_name = 'extracted_wm_complete'\n",
    "cmd = 'fslmaths ' + label_fname + ' -thr ' + str(lower_threshold) + ' -uthr ' + str(upper_threshold) + ' -bin ' + output_name\n",
    "os.system(cmd)\n",
    "load(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7d1f1",
   "metadata": {},
   "source": [
    "Beautiful! So you can see that these labels can be a bit tricky if you're not careful. Based on your above experience above, we will construct a mask in python directly. Please fill in the cell below the two values for:\n",
    "- The white matter regions\n",
    "- The corpus callosum slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b54436",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_callosum_slice_value = ??? # Fill with your value!\n",
    "white_matter_value = ??? # Fill with your value !\n",
    "\n",
    "total_white_matter = (labels == corpus_callosum_slice_value) | (labels == white_matter_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068803fa",
   "metadata": {},
   "source": [
    "#### 2. Actually extracting fiber orientations: the orientation distribution function\n",
    "Okay, now we have a mask to define our fibers. The next cell will be used to estimate the orientation distribution function at each voxel. Before going any further, let's ask why this is necessary. In your opinion, in a single *voxel* how many orientations can we have?\n",
    "- [ ] Exactly one, since only one fiber is passing through the voxel\n",
    "- [ ] One, as the orientation describes the voxel's orientation, not the fibers going through the voxel\n",
    "- [ ] 26, since there are 26 neighbouring voxels with which a link is possible\n",
    "- [ ] As many orientations as there are fibers going through the voxel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a549a23",
   "metadata": {},
   "source": [
    "The issue can be summarized as resolving **intravoxel** fiber orientations of MR images.\n",
    "To summarize these, we use an orientation distribution function, coined ODF.\n",
    "\n",
    "<div class=\\\"warning\\\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "    <span>\n",
    "    <p style='margin-top:1em; text-align:center'><b>💡 Pay attention! 💡</b></p>\n",
    "    <p style='text-indent: 10px;'>\n",
    "        ODF really stands for orientation distribution function here, <b>not</b> ordinary differential function or anything else. Don't confuse the two!</p>\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "We will not bore you with all mathematical details. What you need to know, however, is that this distribution function will rely on a special model, called the constant solid angle ODF model. The idea is the following: considering the distance from origin of the estimated distribution provides useful information. Here's what the solid angle looks like:\n",
    "<img src=\"imgs/tractography/tileshop.jpeg\"/>\n",
    "<center>Left: pdf takes into account solid angle; Right: pdf does not take into account solid angle</center>\n",
    "<i><center>Image taken from Aganj, Iman, et al. \"Reconstruction of the orientation distribution function in single‐and multiple‐shell q‐ball imaging within constant solid angle.\" Magnetic resonance in medicine 64.2 (2010): 554-566.</center></i>\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0241fda",
   "metadata": {},
   "source": [
    "Let's now estimate the orientation distribution function of each voxel, using the CSA-ODF model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.reconst.csdeconv import auto_response_ssst\n",
    "from dipy.reconst.shm import CsaOdfModel\n",
    "from dipy.data import default_sphere\n",
    "from dipy.direction import peaks_from_model\n",
    "\n",
    "# Single fiber response function: the measured signal of a single fiber\n",
    "# sume: regions where there are single coherent fiber populations\n",
    "# auto_response_ssst: calculate FA for a ROI of radii equal to roi_radii in the center of the volume\n",
    "# and return the response function estimated in that region for the voxels with FA higher than 0.7\n",
    "response, ratio = auto_response_ssst(gtab, data, roi_radii=10, fa_thr=0.7)\n",
    "\n",
    "# Instantiate the Constant Solid Angle model\n",
    "csa_model = CsaOdfModel(gtab, sh_order=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b205286",
   "metadata": {},
   "source": [
    "Now that we have our model, the orientation of tract segments can be extracted, looking at the peaks in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c1644",
   "metadata": {},
   "outputs": [],
   "source": [
    "csa_peaks = peaks_from_model(csa_model, data, default_sphere,\n",
    "                             relative_peak_threshold=.8,\n",
    "                             min_separation_angle=45,\n",
    "                             mask=total_white_matter, npeaks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041ed92",
   "metadata": {},
   "source": [
    "Notice in the above cell the following line:\n",
    "```python\n",
    "csa_peaks = peaks_from_model(..., npeaks=5)\n",
    "```\n",
    "\n",
    "This means that really, we extract per voxel five peaks at most. This is an important assumption. Depending on your voxel size, you might want to pay attention to this number!\n",
    "\n",
    "To confirm this, let's have a look at the extracted peak values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "csa_peaks.peak_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef696642",
   "metadata": {},
   "source": [
    "Knowing the MR dimensions, you can see that we indeed have five peaks per voxel. Great! Let's visualize it now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695b081",
   "metadata": {},
   "source": [
    "## Checkpoint: install an additional library\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <p>Please go on the terminal and install the library <b>fury</b> by running:</p>\n",
    "        \n",
    "        conda install -c conda-forge fury\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16fb20",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <i>Make sure you are on the usual environment!</i>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fury import actor, window, ui\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = window.Scene()\n",
    "slice_actor = actor.peak_slicer(csa_peaks.peak_dirs,\n",
    "                            csa_peaks.peak_values,\n",
    "                            affine=affine,mask=total_white_matter,\n",
    "                            colors=None)\n",
    "scene.add(slice_actor)\n",
    "\n",
    "showm = window.ShowManager(scene, size=(900,900), reset_camera=False)\n",
    "showm.initialize()\n",
    "ViewInteractiveWidget(scene.GetRenderWindow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e1250",
   "metadata": {},
   "source": [
    "So as you can see, the orientations do map out to the expected directions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8dd87",
   "metadata": {},
   "source": [
    "### 3. Set the stop criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77906254",
   "metadata": {},
   "source": [
    "Now, we need to setup our fiber tracking to stop it. What criterion should we use?\n",
    "Well, we'll roughly use the idea that when we don't have enough evidence to know where a fiber could have gone, we stop tracking it.\n",
    "In other words, if there are areas where the diffusion is totally unrestricted (goes in all directions), we have no clue as to where the fiber might continue. For this, we can threshold the tendency of our peaks to depend on a specific direction (anisotropy).<br>\n",
    "More specifically, we will threshold the general fractional anisotropy of our data to decide when we should stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc23864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.stopping_criterion import ThresholdStoppingCriterion\n",
    "\n",
    "stopping_criterion = ThresholdStoppingCriterion(csa_peaks.gfa, .25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b904f4b",
   "metadata": {},
   "source": [
    "Let's visualize a slice! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5603d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sli = csa_peaks.gfa.shape[2] // 2\n",
    "plt.figure('GFA')\n",
    "plt.subplot(1, 2, 1).set_axis_off()\n",
    "plt.imshow(csa_peaks.gfa[:, :, sli].T, cmap='gray', origin='lower')\n",
    "\n",
    "plt.subplot(1, 2, 2).set_axis_off()\n",
    "plt.imshow((csa_peaks.gfa[:, :, sli] > 0.25).T, cmap='gray', origin='lower')\n",
    "\n",
    "plt.savefig('gfa_tracking_mask.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5117d0",
   "metadata": {},
   "source": [
    "### 4. Specify where to begin the fibers tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2101ba67",
   "metadata": {},
   "source": [
    "There are different ways to place seeds, ie starting points from which the fiber tracking is started. This depends on the pathways you might like to model! For example, if you only wanted to model the corpus callosum it would not be so interesting to place seeds in other regions of the brain. <br>\n",
    "So that you understand what the output of the cell below will be, we must first explain what you'll extract in the cell below.<br>\n",
    "The orientation of an image is described by its affine transformation, if you remember well. Let's call this affine $A$.\n",
    "A seed point at the center of voxel $[i,j,k]$ will be represented as $[x,y,z, 1]= A \\cdot [i,j,k,1]$<br>\n",
    "In other words, you will get **coordinates in voxel space**. Note that there is one important assumption: the voxels here should be isotropic (the size of a voxel should be same along all directions).\n",
    "\n",
    "In our specific case, we will start from a sagittal slice of the **corpus callosum**, the one with label 2 to be specific.\n",
    "Please, create the mask (based on the labels above) to extract **only the slice of corpus callosum with label 2 as a mask**. You can refer to what we did above to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking import utils\n",
    "\n",
    "seed_mask = ??? # Your code here to extract only the place of interest! \n",
    "seeds = utils.seeds_from_mask(seed_mask, affine, density=[2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277de50",
   "metadata": {},
   "source": [
    "### 5. Bringing it all together and generating the streamlines\n",
    "Let's see our ingredients:\n",
    "- Seeds, generated above and starting from a slice of the corpus callosum\n",
    "- A mask of regions where we should stop our fibers, based on anisotropy\n",
    "- Peaks of ODF, at most five peaks per voxel\n",
    "\n",
    "It remains now to combine all of these to bake so called streamlines (ie: fibers!). To do so, we will use the EuDX algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.local_tracking import LocalTracking\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "\n",
    "# Initialization of LocalTracking. The computation happens in the next step.\n",
    "streamlines_generator = LocalTracking(csa_peaks, stopping_criterion, seeds,\n",
    "                                      affine=affine, step_size=.5)\n",
    "# Generate streamlines object\n",
    "streamlines = Streamlines(streamlines_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef1109",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07184cd0",
   "metadata": {},
   "source": [
    "Beautiful! Let's now visualize our streamlines!\n",
    "Remember that they represent **only lines that start from the corpus callosum**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da572e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.viz import colormap\n",
    "\n",
    "# Prepare the display objects.\n",
    "color = colormap.line_colors(streamlines)\n",
    "\n",
    "streamlines_actor = actor.line(streamlines,\n",
    "                               colormap.line_colors(streamlines))\n",
    "\n",
    "# Create the 3D display.\n",
    "scene = window.Scene()\n",
    "scene.add(streamlines_actor)\n",
    "\n",
    "showm = window.ShowManager(scene, size=(900,900), reset_camera=False)\n",
    "showm.initialize()\n",
    "ViewInteractiveWidget(scene.GetRenderWindow())\n",
    "\n",
    "# Save still images for this static example. Or for interactivity use\n",
    "#window.record(scene, out_path='tractogram_EuDX.png', size=(800, 800))\n",
    "#if interactive:\n",
    "#    window.show(scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd927bf",
   "metadata": {},
   "source": [
    "### 6. The impact of seeds\n",
    "\n",
    "If you remember, we did state that our seeds be restricted only to the slice of interest. Now, what would happen if we went for the full white matter? Obviously it would be more expensive, but let's do it for the sake of curiosity nonetheless! (Note that the display function might struggle a bit, that's fine)### 6. Store the streamlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4de732",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_t = utils.seeds_from_mask(total_white_matter, affine, density=[2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2455e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of LocalTracking. The computation happens in the next step.\n",
    "streamlines_generator_t = LocalTracking(csa_peaks, stopping_criterion, seeds_t,\n",
    "                                      affine=affine, step_size=.5)\n",
    "# Generate streamlines object\n",
    "streamlines_t = Streamlines(streamlines_generator_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the display objects.\n",
    "color = colormap.line_colors(streamlines_t)\n",
    "\n",
    "streamlines_actor = actor.line(streamlines_t,\n",
    "                               colormap.line_colors(streamlines_t))\n",
    "\n",
    "# Create the 3D display.\n",
    "scene = window.Scene()\n",
    "scene.add(streamlines_actor)\n",
    "\n",
    "showm = window.ShowManager(scene, size=(900,900), reset_camera=False)\n",
    "showm.initialize()\n",
    "ViewInteractiveWidget(scene.GetRenderWindow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05149fea",
   "metadata": {},
   "source": [
    "### 7. Store the streamlines into a trackvis file\n",
    "\n",
    "What if we wanted to save the result as a file? Well, you can! For this, we need to save it to a special format, the TrackVis (.trk) format.\n",
    "\n",
    "Remember: our goal was to generate the streamlines. It is these streamlines that we therefore want to save! :) \n",
    "Let's do it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f052b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.io.stateful_tractogram import Space, StatefulTractogram\n",
    "from dipy.io.streamline import save_tractogram, save_trk\n",
    "\n",
    "# This is for the cc slice tractogram\n",
    "sft = StatefulTractogram(streamlines, hardi_img, Space.RASMM)\n",
    "save_trk(sft, \"tractogram_EuDX.trk\", streamlines)\n",
    "\n",
    "# This is for the whole wm tractogram\n",
    "sft_t = StatefulTractogram(streamlines_t, hardi_img, Space.RASMM)\n",
    "save_trk(sft_t, \"tractogram_full_wm_EuDX.trk\", streamlines_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61be9e0",
   "metadata": {},
   "source": [
    "If you want to visualize it all, you can install <a href=\"http://trackvis.org/download/\">TrackVis</a> and open the file from within! (TrackVis is free, no worries). It will be interactive and in 3D! Pretty cool huh?<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5cc30f",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Let's sum up what we've seen. For a successfull tractography generation, we need the following: \n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Ingredient</th>\n",
    "        <th>Role</th>\n",
    "        <th>How is it created?</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Seeds</td>\n",
    "        <td>Define starting point of tract propagation.</td>\n",
    "        <td>Can be done randomly or according to some mask of interest</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Diffusion directions</td>\n",
    "        <td>Define the local diffusion in a voxel, for all voxels of interest</td>\n",
    "        <td>Can be done with CSA-ODF or other methods such as e.g structure tensor</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Stopping criteria</td>\n",
    "        <td>Defines where the tract continues or stops.</td>\n",
    "        <td>Can be done based on anatomy, information of diffusion direction, combination of both...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>A tracking algorithm</td>\n",
    "        <td>Combines all ingredients above to generate streamlines</td>\n",
    "        <td>Line propagation techniques to grow from seed region, or probabilistic with a pdf of fiber orientations.</td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "<img src=\"imgs/tractography/4qamin.jpg\"/>\n",
    "\n",
    "Each of the ingredients can be changed for a different flavour. You can explore <a href=\"https://dipy.org/tutorials/\">DIPY's tutorials</a> to get an idea of the changes you can operate. Feel free to play around!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ffc0a",
   "metadata": {},
   "source": [
    "# Part 3: Connectivity analysis based on tractography "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc0f481",
   "metadata": {},
   "source": [
    "Now that you've seen how to generate streamlines, we will look at their analysis. \n",
    "For example, how do we find the streamlines those pass through or do not pass through some region of the brain? How do we count streamlines based on a start and end point in the brain? How do we count the number of streamlines that pass through each voxel of some image?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dc3edb",
   "metadata": {},
   "source": [
    "### 1. Generating the streamlines to work with\n",
    "\n",
    "Obviously, the first part to analyse streamlines is...to have streamlines!\n",
    "For this, we refer you to the previous part on tractography. What we will do here is create a tractography with seeds spanning the entire white matter. As you've already saved the streamlines as a file, we will load it directly! To do so, we need a reference anatomy (have a look <a href=\"https://dipy.org/documentation/1.5.0/examples_built/streamline_formats/\">here</a> for a more complete treatment of DIPY's loading philosophy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.io.streamline import load_tractogram\n",
    "from dipy.core.gradients import gradient_table\n",
    "from dipy.data import get_fnames\n",
    "from dipy.io.gradients import read_bvals_bvecs\n",
    "from dipy.io.image import load_nifti, load_nifti_data\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "hardi_fname, _, _ = get_fnames('stanford_hardi')\n",
    "reference_anatomy = nib.load(hardi_fname)\n",
    "t1_fname = get_fnames('stanford_t1')\n",
    "t1_data = load_nifti_data(t1_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b904d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tractogram = load_tractogram(\"tractogram_full_wm_EuDX.trk\", reference_anatomy)\n",
    "\n",
    "# Important: some stream lines might be invalid. Let's rid ourselves of them :)\n",
    "tractogram.remove_invalid_streamlines()\n",
    "streamlines = tractogram.streamlines\n",
    "affine = tractogram.affine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13266df",
   "metadata": {},
   "source": [
    "### 2.1 Count the number of streamlines passing through a given ROI\n",
    "\n",
    "How many streamlines pass through the slice of corpus callosum you saw in the previous section? To figure this out, we simply need to extract back the corpus callosum mask from the label file. Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get back the mask\n",
    "label_fname = get_fnames('stanford_labels')\n",
    "labels = load_nifti_data(label_fname)\n",
    "\n",
    "# generate the mask for the sagittal slice of corpus callosum\n",
    "cc_slice = ??? # Put here your code to do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6824303",
   "metadata": {},
   "source": [
    "Now, we will use a function from utils, the target function.\n",
    "It takes a set of streamlines, a region of interest (ROI) and a boolean flag.\n",
    "If the flag is set to true, the function returns *only* the streamlines passing through the mask.\n",
    "If the flag is set to false, it returns *only* streamlines not passing through the mask.\n",
    "\n",
    "Obviously, these two results should sum up to exactly our number of streamlines, otherwise we have an issue! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d13f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking import utils\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "\n",
    "# These are our streamlines of interest: those going through our mask!\n",
    "cc_streamlines = utils.target(streamlines, affine, cc_slice)\n",
    "cc_streamlines = Streamlines(cc_streamlines)\n",
    "\n",
    "# These are all other streamlines\n",
    "other_streamlines = utils.target(streamlines, affine, cc_slice,\n",
    "                                 include=False)\n",
    "other_streamlines = Streamlines(other_streamlines)\n",
    "\n",
    "# Just a quick check: the two sets of streamlines should sum up to exactly the number of streamlines!\n",
    "assert len(other_streamlines) + len(cc_streamlines) == len(streamlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d53069",
   "metadata": {},
   "source": [
    "So, how many streamlines pass through the slice of corpus callosum?\n",
    "- [ ] Half of all streamlines\n",
    "- [ ] All of them\n",
    "- [ ] None of them\n",
    "- [ ] About ten percent of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0de3a",
   "metadata": {},
   "source": [
    "For the sake of completeness, how about visualizing the fibers that pass through the slice of corpus callosum? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fury import actor, window, ui, colormap as cmap\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "\n",
    "color = cmap.line_colors(cc_streamlines)\n",
    "cc_streamlines_actor = actor.line(cc_streamlines,\n",
    "                                  cmap.line_colors(cc_streamlines))\n",
    "cc_ROI_actor = actor.contour_from_roi(cc_slice, color=(1., 1., 0.),\n",
    "                                      opacity=0.5, affine=affine)\n",
    "\n",
    "vol_actor = actor.slicer(t1_data, affine)\n",
    "\n",
    "vol_actor.display(x=40)\n",
    "vol_actor2 = vol_actor.copy()\n",
    "vol_actor2.display(z=35)\n",
    "\n",
    "# Add display objects to canvas\n",
    "scene = window.Scene()\n",
    "scene.add(vol_actor)\n",
    "scene.add(vol_actor2)\n",
    "scene.add(cc_streamlines_actor)\n",
    "scene.add(cc_ROI_actor)\n",
    "\n",
    "showm = window.ShowManager(scene, size=(900,900), reset_camera=False)\n",
    "showm.initialize()\n",
    "ViewInteractiveWidget(scene.GetRenderWindow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421175c",
   "metadata": {},
   "source": [
    "### 2.2 Finding out which regions of the brain are connected by these streamlines\n",
    "\n",
    "Now we'll use the streamlines to build a connectivity matrix. Why? The idea is simple: we want to know where streamlines start and where they end in the brain. Since the brain is labelled, we can figure this out by alternatively asking how many fibers go from a region to another. Answer the following to build up some intuition:\n",
    "\n",
    "- [ ] We are modelling the brain as a graph. The nodes are the fibers, and the edges are the regions.\n",
    "- [ ] We are modelling the brain as a graph. The nodes are the regions, and the edges are the number of fibers between pair of nodes.\n",
    "\n",
    "Now, all is good and well. But let's go a bit deeper. If we count the number of fibers going from region A to B, for all pairs of region, and we set up our seeds to grow from region R to all other regions, what do you expect to happen?\n",
    "- [ ] Region R's line in the connectivity matrix will describe all tracts of the tractogram, because they exclusively grow from it to other regions?\n",
    "- [ ] All regions except region R will have zero tract going to other regions, because they were not used as seed to grow tractograms from?\n",
    "- [ ] We cannot accurately decide without extensive knowledge of the tracking algorithm, the seed mask itself is an insufficient information to know if there is any directionality involved in the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e33d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connectivity_matrix: takes a set of streamlines and an array of labels as arguments\n",
    "# returns the number of streamlines that start and end at each pair of labels and return streamlines grouped by their endpoints\n",
    "M, grouping = utils.connectivity_matrix([sl[0::len(sl)-1] for sl in cc_streamlines if len(sl)-1 > 0], affine,\n",
    "                                        labels.astype(np.uint8),\n",
    "                                        return_mapping=True,\n",
    "                                        mapping_as_streamlines=True)\n",
    "# we are interested in connections between GM regions, label 0 represents background and 1,2 represent WM\n",
    "# DISCARD THE FIRST THREE ROW AND COLUMNS OF THE CONNECTIVITY MATRIX\n",
    "M[:3, :] = 0\n",
    "M[:, :3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the matrix by matplotlib\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(np.log1p(M), interpolation='nearest')\n",
    "plt.savefig(\"connectivity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfaf48",
   "metadata": {},
   "source": [
    "Let's find the maximum (besides diagonal elements), and where they are located: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef706a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(M == np.max(M-np.eye(M.shape[0]) * np.diag(M)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b35f36",
   "metadata": {},
   "source": [
    "Remember we are dealing here with **labels**. We can extract the label infos, straight from the label file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd689dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import dirname, abspath\n",
    "import os.path as op\n",
    "\n",
    "region_labels = pd.read_csv(op.join(dirname(abspath(label_fname)), 'label_info.txt'))\n",
    "region_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17ff21",
   "metadata": {},
   "source": [
    "Let's find the region with label 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbf510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_labels[region_labels[\"new label\"] == ???] # set the proper value here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982857e1",
   "metadata": {},
   "source": [
    "And the region with label 54 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8953bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_labels[region_labels[\"new label\"] == ???] # set the proper value here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1be2f",
   "metadata": {},
   "source": [
    "So, more tracts connect the left superior frontal gyrus and the right superior frontal gyrus than other pair of regions. In this particular case, the matrix was symetric. Again remember: the exact interpretation is **hard** and a very active area of research, so you'll need to really be sure of what your tracking algorithm does before making any claim :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e4c4b",
   "metadata": {},
   "source": [
    "### 2.3 Representing the spatial distribution of a track by counting the density of streamlines in each voxel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_superiorfrontal_track = grouping[11, 54]\n",
    "shape = labels.shape\n",
    "dm = utils.density_map(lr_superiorfrontal_track, affine, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6776f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.io.stateful_tractogram import Space, StatefulTractogram\n",
    "from dipy.io.streamline import save_trk\n",
    "from dipy.io.image import load_nifti_data, load_nifti, save_nifti\n",
    "\n",
    "# Save density map\n",
    "save_nifti(\"lr-superiorfrontal-dm.nii.gz\", dm.astype(\"int16\"), affine)\n",
    "\n",
    "lr_sf_trk = Streamlines(lr_superiorfrontal_track)\n",
    "\n",
    "# Save streamlines\n",
    "_, _, hardi_img = load_nifti(hardi_fname, return_img=True)\n",
    "sft = StatefulTractogram(lr_sf_trk, hardi_img, Space.VOX)\n",
    "save_trk(sft, \"lr-superiorfrontal.trk\", bbox_valid_check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c348f305",
   "metadata": {},
   "source": [
    "This ends this short tractography tutorial! Again, do not hesitate to explore more on DIPY's website if you're interested :)\n",
    "We strongly encourage you to use TrackVis for visualization of tractograms as it's really made for it and is much more intuitive to use for this purpose than FSLeyes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac02d6a-d896-48e0-a36c-e280f83b37df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<p><b>Congratulations for having finished this part of the laboratory! </b></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
